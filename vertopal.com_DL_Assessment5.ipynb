{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Lab - BCSE332P<a href=\"#Deep-Learning-Lab---BCSE332P\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "# By - Harsh Agarwal (21BAI1481)<a href=\"#By---Harsh-Agawral-(21BAI1481)\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "# Assessment 5<a href=\"#Assessment-5\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "## Implementation Of RNN and LSTM With And Without Dense Layers<a href=\"#Implementation-Of-RNN-and-LSTM-With-And-Without-Dense-Layers\"\n",
    "class=\"anchor-link\">¶</a>\n",
    "\n",
    "Link - <https://ai.stanford.edu/~amaas/data/sentiment/>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
    "    from tensorflow.strings import regex_replace\n",
    "    from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras import Input\n",
    "    from tensorflow.keras.layers import Embedding, Dense, LSTM, SimpleRNN as RNN\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    import tarfile\n",
    "    import os\n",
    "    tar_file_path = '/content/drive/MyDrive/DL Lab/Datasets/aclImdb_v1.tar.gz'\n",
    "    extract_path = '/content/'\n",
    "    with tarfile.open(tar_file_path, 'r:gz') as tar_ref:\n",
    "        tar_ref.extractall(extract_path)\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    def prepareData(dir):\n",
    "        data = text_dataset_from_directory(dir)\n",
    "        return data.map(lambda text, label: (regex_replace(text, '<br />', ' '), label))\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    rm -rf \"/content/aclImdb/train/unsup\"\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    train_data = prepareData('/content/aclImdb/train')\n",
    "    test_data = prepareData('/content/aclImdb/test')\n",
    "\n",
    "    Found 25000 files belonging to 2 classes.\n",
    "    Found 25000 files belonging to 2 classes.\n",
    "\n",
    "## RNN Without Dense Layers<a href=\"#RNN-Without-Dense-Layers\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    model_rnn = Sequential()\n",
    "    model_rnn.add(Input(shape=(1,), dtype=\"string\"))\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    max_tokens = 1000\n",
    "    max_len = 100\n",
    "    vectorize_layer = TextVectorization(\n",
    "        max_tokens=max_tokens,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=max_len,\n",
    "    )\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    train_texts = train_data.map(lambda text, label: text)\n",
    "    vectorize_layer.adapt(train_texts)\n",
    "    model_rnn.add(vectorize_layer)\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    model_rnn.add(Embedding(max_tokens + 1, 128))\n",
    "    model_rnn.add(RNN(64, return_sequences=True))\n",
    "    model_rnn.add(RNN(32, return_sequences=True))\n",
    "    model_rnn.add(RNN(1, activation=\"sigmoid\"))\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    model_rnn.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model_rnn.summary()\n",
    "\n",
    "    Model: \"sequential\"\n",
    "    _________________________________________________________________\n",
    "     Layer (type)                Output Shape              Param #   \n",
    "    =================================================================\n",
    "     text_vectorization (TextVe  (None, 100)               0         \n",
    "     ctorization)                                                    \n",
    "                                                                     \n",
    "     embedding (Embedding)       (None, 100, 128)          128128    \n",
    "                                                                     \n",
    "     simple_rnn (SimpleRNN)      (None, 100, 64)           12352     \n",
    "                                                                     \n",
    "     simple_rnn_1 (SimpleRNN)    (None, 100, 32)           3104      \n",
    "                                                                     \n",
    "     simple_rnn_2 (SimpleRNN)    (None, 1)                 34        \n",
    "                                                                     \n",
    "    =================================================================\n",
    "    Total params: 143618 (561.01 KB)\n",
    "    Trainable params: 143618 (561.01 KB)\n",
    "    Non-trainable params: 0 (0.00 Byte)\n",
    "    _________________________________________________________________\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    model_rnn.fit(train_data, epochs=10)\n",
    "\n",
    "    Epoch 1/10\n",
    "    782/782 [==============================] - 62s 75ms/step - loss: 0.6951 - accuracy: 0.5048\n",
    "    Epoch 2/10\n",
    "    782/782 [==============================] - 60s 77ms/step - loss: 0.6879 - accuracy: 0.5508\n",
    "    Epoch 3/10\n",
    "    782/782 [==============================] - 60s 76ms/step - loss: 0.6605 - accuracy: 0.6117\n",
    "    Epoch 4/10\n",
    "    782/782 [==============================] - 59s 76ms/step - loss: 0.6623 - accuracy: 0.6070\n",
    "    Epoch 5/10\n",
    "    782/782 [==============================] - 59s 75ms/step - loss: 0.6648 - accuracy: 0.5879\n",
    "    Epoch 6/10\n",
    "    782/782 [==============================] - 58s 74ms/step - loss: 0.6564 - accuracy: 0.6056\n",
    "    Epoch 7/10\n",
    "    782/782 [==============================] - 58s 75ms/step - loss: 0.6340 - accuracy: 0.6512\n",
    "    Epoch 8/10\n",
    "    782/782 [==============================] - 58s 74ms/step - loss: 0.6513 - accuracy: 0.6186\n",
    "    Epoch 9/10\n",
    "    782/782 [==============================] - 58s 74ms/step - loss: 0.6593 - accuracy: 0.6078\n",
    "    Epoch 10/10\n",
    "    782/782 [==============================] - 60s 76ms/step - loss: 0.6330 - accuracy: 0.6529\n",
    "\n",
    "Out\\[ \\]:\n",
    "\n",
    "    <keras.src.callbacks.History at 0x7bdc696e0eb0>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    model_rnn.evaluate(test_data)\n",
    "\n",
    "    782/782 [==============================] - 19s 24ms/step - loss: 0.6118 - accuracy: 0.6912\n",
    "\n",
    "Out\\[ \\]:\n",
    "\n",
    "    [0.6118454933166504, 0.6912000179290771]\n",
    "\n",
    "## LSTM Without Dense Layers<a href=\"#LSTM-Without-Dense-Layers\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    model_lstm = Sequential()\n",
    "    model_lstm.add(Input(shape=(1,), dtype=\"string\"))\n",
    "    model_lstm.add(vectorize_layer)\n",
    "    model_lstm.add(Embedding(max_tokens + 1, 128))\n",
    "    model_lstm.add(LSTM(64, return_sequences=True))\n",
    "    model_lstm.add(LSTM(32, return_sequences=True))\n",
    "    model_lstm.add(LSTM(1, activation=\"sigmoid\"))\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    model_lstm.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model_lstm.summary()\n",
    "\n",
    "    Model: \"sequential_1\"\n",
    "    _________________________________________________________________\n",
    "     Layer (type)                Output Shape              Param #   \n",
    "    =================================================================\n",
    "     text_vectorization (TextVe  (None, 100)               0         \n",
    "     ctorization)                                                    \n",
    "                                                                     \n",
    "     embedding_1 (Embedding)     (None, 100, 128)          128128    \n",
    "                                                                     \n",
    "     lstm (LSTM)                 (None, 100, 64)           49408     \n",
    "                                                                     \n",
    "     lstm_1 (LSTM)               (None, 100, 32)           12416     \n",
    "                                                                     \n",
    "     lstm_2 (LSTM)               (None, 1)                 136       \n",
    "                                                                     \n",
    "    =================================================================\n",
    "    Total params: 190088 (742.53 KB)\n",
    "    Trainable params: 190088 (742.53 KB)\n",
    "    Non-trainable params: 0 (0.00 Byte)\n",
    "    _________________________________________________________________\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    model_lstm.fit(train_data, epochs=10)\n",
    "\n",
    "    Epoch 1/10\n",
    "    782/782 [==============================] - 121s 147ms/step - loss: 0.5436 - accuracy: 0.7169\n",
    "    Epoch 2/10\n",
    "    782/782 [==============================] - 115s 147ms/step - loss: 0.4437 - accuracy: 0.7975\n",
    "    Epoch 3/10\n",
    "    782/782 [==============================] - 115s 147ms/step - loss: 0.4218 - accuracy: 0.8082\n",
    "    Epoch 4/10\n",
    "    782/782 [==============================] - 115s 147ms/step - loss: 0.3966 - accuracy: 0.8210\n",
    "    Epoch 5/10\n",
    "    782/782 [==============================] - 115s 147ms/step - loss: 0.4185 - accuracy: 0.8129\n",
    "    Epoch 6/10\n",
    "    782/782 [==============================] - 116s 148ms/step - loss: 0.3901 - accuracy: 0.8270\n",
    "    Epoch 7/10\n",
    "    782/782 [==============================] - 115s 147ms/step - loss: 0.3676 - accuracy: 0.8359\n",
    "    Epoch 8/10\n",
    "    782/782 [==============================] - 115s 147ms/step - loss: 0.3543 - accuracy: 0.8429\n",
    "    Epoch 9/10\n",
    "    782/782 [==============================] - 117s 149ms/step - loss: 0.3516 - accuracy: 0.8456\n",
    "    Epoch 10/10\n",
    "    782/782 [==============================] - 116s 148ms/step - loss: 0.3288 - accuracy: 0.8591\n",
    "\n",
    "Out\\[ \\]:\n",
    "\n",
    "    <keras.src.callbacks.History at 0x7bdc6975a230>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    model_lstm.evaluate(test_data)\n",
    "\n",
    "    782/782 [==============================] - 33s 41ms/step - loss: 0.4612 - accuracy: 0.7936\n",
    "\n",
    "Out\\[ \\]:\n",
    "\n",
    "    [0.46123409271240234, 0.7936000227928162]\n",
    "\n",
    "## RNN With Dense Layers (Done In Lab)<a href=\"#RNN-With-Dense-Layers-(Done-In-Lab)\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    model_rnn_dense = Sequential()\n",
    "    model_rnn_dense.add(Input(shape=(1,), dtype=\"string\"))\n",
    "    model_rnn_dense.add(vectorize_layer)\n",
    "    model_rnn_dense.add(Embedding(max_tokens + 1, 128))\n",
    "    model_rnn_dense.add(RNN(64, return_sequences=False))\n",
    "    model_rnn_dense.add(Dense(64, activation=\"relu\"))\n",
    "    model_rnn_dense.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    model_rnn_dense.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model_rnn_dense.summary()\n",
    "\n",
    "    Model: \"sequential_5\"\n",
    "    _________________________________________________________________\n",
    "     Layer (type)                Output Shape              Param #   \n",
    "    =================================================================\n",
    "     text_vectorization (TextVe  (None, 100)               0         \n",
    "     ctorization)                                                    \n",
    "                                                                     \n",
    "     embedding_5 (Embedding)     (None, 100, 128)          128128    \n",
    "                                                                     \n",
    "     simple_rnn_6 (SimpleRNN)    (None, 64)                12352     \n",
    "                                                                     \n",
    "     dense_6 (Dense)             (None, 64)                4160      \n",
    "                                                                     \n",
    "     dense_7 (Dense)             (None, 1)                 65        \n",
    "                                                                     \n",
    "    =================================================================\n",
    "    Total params: 144705 (565.25 KB)\n",
    "    Trainable params: 144705 (565.25 KB)\n",
    "    Non-trainable params: 0 (0.00 Byte)\n",
    "    _________________________________________________________________\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    model_rnn_dense.fit(train_data, epochs=10)\n",
    "\n",
    "    Epoch 1/10\n",
    "    782/782 [==============================] - 31s 38ms/step - loss: 0.6951 - accuracy: 0.5152\n",
    "    Epoch 2/10\n",
    "    782/782 [==============================] - 29s 37ms/step - loss: 0.6898 - accuracy: 0.5357\n",
    "    Epoch 3/10\n",
    "    782/782 [==============================] - 29s 37ms/step - loss: 0.6810 - accuracy: 0.5654\n",
    "    Epoch 4/10\n",
    "    782/782 [==============================] - 29s 37ms/step - loss: 0.6676 - accuracy: 0.5893\n",
    "    Epoch 5/10\n",
    "    782/782 [==============================] - 29s 37ms/step - loss: 0.6599 - accuracy: 0.6011\n",
    "    Epoch 6/10\n",
    "    782/782 [==============================] - 29s 37ms/step - loss: 0.6592 - accuracy: 0.5987\n",
    "    Epoch 7/10\n",
    "    782/782 [==============================] - 29s 37ms/step - loss: 0.6461 - accuracy: 0.6275\n",
    "    Epoch 8/10\n",
    "    782/782 [==============================] - 29s 37ms/step - loss: 0.6487 - accuracy: 0.6114\n",
    "    Epoch 9/10\n",
    "    782/782 [==============================] - 29s 37ms/step - loss: 0.6286 - accuracy: 0.6390\n",
    "    Epoch 10/10\n",
    "    782/782 [==============================] - 29s 37ms/step - loss: 0.6412 - accuracy: 0.6240\n",
    "\n",
    "Out\\[ \\]:\n",
    "\n",
    "    <keras.src.callbacks.History at 0x7bdc59e1c430>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    model_rnn_dense.evaluate(test_data)\n",
    "\n",
    "    782/782 [==============================] - 11s 14ms/step - loss: 0.6984 - accuracy: 0.5434\n",
    "\n",
    "Out\\[ \\]:\n",
    "\n",
    "    [0.6983756422996521, 0.5434399843215942]\n",
    "\n",
    "## LSTM With Dense Layers (Done In Lab)<a href=\"#LSTM-With-Dense-Layers-(Done-In-Lab)\"\n",
    "class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    model_lstm_dense = Sequential()\n",
    "    model_lstm_dense.add(Input(shape=(1,), dtype=\"string\"))\n",
    "    model_lstm_dense.add(vectorize_layer)\n",
    "    model_lstm_dense.add(Embedding(max_tokens + 1, 128))\n",
    "    model_lstm_dense.add(LSTM(64, return_sequences=False))\n",
    "    model_lstm_dense.add(Dense(32, activation=\"relu\"))\n",
    "    model_lstm_dense.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    model_lstm_dense.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model_lstm_dense.summary()\n",
    "\n",
    "    Model: \"sequential_6\"\n",
    "    _________________________________________________________________\n",
    "     Layer (type)                Output Shape              Param #   \n",
    "    =================================================================\n",
    "     text_vectorization (TextVe  (None, 100)               0         \n",
    "     ctorization)                                                    \n",
    "                                                                     \n",
    "     embedding_6 (Embedding)     (None, 100, 128)          128128    \n",
    "                                                                     \n",
    "     lstm_3 (LSTM)               (None, 64)                49408     \n",
    "                                                                     \n",
    "     dense_8 (Dense)             (None, 32)                2080      \n",
    "                                                                     \n",
    "     dense_9 (Dense)             (None, 1)                 33        \n",
    "                                                                     \n",
    "    =================================================================\n",
    "    Total params: 179649 (701.75 KB)\n",
    "    Trainable params: 179649 (701.75 KB)\n",
    "    Non-trainable params: 0 (0.00 Byte)\n",
    "    _________________________________________________________________\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    model_lstm_dense.fit(train_data, epochs=10)\n",
    "\n",
    "    Epoch 1/10\n",
    "    782/782 [==============================] - 62s 76ms/step - loss: 0.5319 - accuracy: 0.7292\n",
    "    Epoch 2/10\n",
    "    782/782 [==============================] - 59s 75ms/step - loss: 0.4394 - accuracy: 0.7992\n",
    "    Epoch 3/10\n",
    "    782/782 [==============================] - 59s 75ms/step - loss: 0.4157 - accuracy: 0.8118\n",
    "    Epoch 4/10\n",
    "    782/782 [==============================] - 59s 75ms/step - loss: 0.3873 - accuracy: 0.8267\n",
    "    Epoch 5/10\n",
    "    782/782 [==============================] - 58s 74ms/step - loss: 0.3746 - accuracy: 0.8329\n",
    "    Epoch 6/10\n",
    "    782/782 [==============================] - 59s 75ms/step - loss: 0.3518 - accuracy: 0.8462\n",
    "    Epoch 7/10\n",
    "    782/782 [==============================] - 60s 76ms/step - loss: 0.3444 - accuracy: 0.8502\n",
    "    Epoch 8/10\n",
    "    782/782 [==============================] - 59s 75ms/step - loss: 0.3315 - accuracy: 0.8577\n",
    "    Epoch 9/10\n",
    "    782/782 [==============================] - 59s 75ms/step - loss: 0.3222 - accuracy: 0.8618\n",
    "    Epoch 10/10\n",
    "    782/782 [==============================] - 59s 75ms/step - loss: 0.3087 - accuracy: 0.8671\n",
    "\n",
    "Out\\[ \\]:\n",
    "\n",
    "    <keras.src.callbacks.History at 0x7bdc514858a0>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    model_lstm_dense.evaluate(test_data)\n",
    "\n",
    "    782/782 [==============================] - 20s 25ms/step - loss: 0.5160 - accuracy: 0.7771\n",
    "\n",
    "Out\\[ \\]:\n",
    "\n",
    "    [0.5160044431686401, 0.7770799994468689]"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
